# Deep Learning Dictionary

Written by Yixiong Ding / YixiongDing@Gmail.com
June, 2022
_ _ _
## A
- ### Activation Function 激活函数
  1. 激活函数的用途是将一个无边界的输入，转变成一个可预测的形式。
  2. 最主要的作用是给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

## B
- ### Back Propagation 反向传播算法
- ### Bias 偏差、截距项
  偏差是除了权重之外，另一个被应用于输入数据的线性分量。它被加到权重与输入数据相乘的结果中，用于改变权重与输入相乘所得结果的范围。
  ![](2022-06-19-22-50-40.png)

## F
- ### Feature Pyramid Network (FPN) 特征金字塔网络
  1. 目标检测卷积结构类型


- ### Feedforward 前馈神经网络
  1. 输入，经过加权，加上偏差，经过激活函数，输入在神经网络中向前传输，最终得到输出。这样的过程被称为前馈.
   ![](2022-06-16-11-56-08.png)

## H
- ### Hyper Parameter 超参数
   超参数是我们控制我们模型结构、功能、效率等的调节旋钮，具体有：

  1. Learning Rate
  2. Epochs(迭代次数，也可称为 num of iterations)
  3. Num of Hidden Layers(隐层数目)
  4. Num of Hidden Layer Units(隐层的单元数/神经元数)
  5. Activation Function(激活函数)
  6. Batch-Size(用mini-batch SGD的时候每个批量的大小)
  7. Optimizer(选择什么优化器，如SGD、RMSProp、Adam)
  8. 用诸如RMSProp、Adam优化器的时候涉及到的β1，β2等等
   
   
   上面是一些最常见的超参数，一般的深度学习框架就是调节这些玩意儿。
   具体怎么调节，在不同的场景中基本都不同，没有统一的标准说learning rate取多少比较好、epochs多少比较好，都是在在实际情况中反复试验的。当然，如果我们可以借鉴一些知名的项目的超参数的选择，来应用到我们类似的项目中去。

## L
- ### Learning Rate 学习率
  1. 学习率是一个常数，存在于各种优化算法中，用于调整训练的速度。


- ### Loss Function 损失函数 
  1. 量化网络结构的优良性，从而可以寻找更好的网络。
   ![](2022-06-16-17-49-11.png)
  2. 也可以把损失看成是权重（weight）和截距项（bias）的函数。
   ![](2022-06-16-18-16-25.png)

## O
- ### Operator 算子
   深度学习算法由一个个计算单元组成，我们称这些计算单元为算子（Operator，简称Op）。在网络模型中，算子对应层中的计算逻辑，例如：卷积层（Convolution Layer）是一个算子；全连接层（Fully-connected Layer， FC layer）中的权值求和过程，是一个算子。

## P

- ### Parameter 参数
   参数是我们训练神经网络 最终要学习的目标，最基本的就是神经网络的权重Weight和截距项bias，我们训练的目的，就是要找到一套好的模型参数，用于预测未知的结果。这些参数我们是不用调的，是模型来训练的过程中自动更新生成的。

- ### Prymaid Network
  1. 目标检测算法常用结构。
  2. 通过将输入图像缩放到不同尺度的大小构成了图像金字塔。然后将这些不同尺度的特征输入到网络中（可以共享参数也可以独立参数）。
  3. 得到每个尺度的检测结果，然后通过NMS等后处理手段进行预测结果的处理。
  4. 图像金字塔最大的问题是推理速度慢了几倍，一个是因为要推理的图像数多了几倍，另一个原因是要检测小目标势必要放大图像。

## S 

- ### Stochastic Gradient Descent SGD 随机梯度下降
  1. 网络优化算法，优化网络的权重和截距项，实现损失的最小化。

## T 

- ### Tensor 张量
   Tensor是算子中的数据，包括输入数据与输出数据

- ### Training Process 训练过程
   对网络中的每个权重（weight）和截距项（bias）进行优化，损失就会不断下降，网络性能会不断上升。训练过程是这样的： 
  1. 从数据集中选择一个样本，用随机梯度下降法进行优化 —— 每次都只针对一个样本进行优化； 
  2. 计算每个权重或截距项对损失的偏导；
  3. 用更新等式更新每个权重和截距项；
  4. 在重复第一步；

## W
- ### Weight 权重
  当输入数据进入计算单元时，会乘以一个权重。例如，如果一个算子有两个输入，则每个输入会分配一个关联权重，一般将认为较重要数据赋予较高的权重，不重要的数据赋予较小的权重，为零的权重则表示特定的特征是无需关注的。
  ![](2022-06-19-22-48-26.png)